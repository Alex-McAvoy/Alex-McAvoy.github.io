<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/img/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicon-16x16-next.png">
  <link rel="mask-icon" href="/assets/img/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script>



<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"alex-mcavoy.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":16,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#ff0000","save":"manual"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="Reference  详解最大熵模型 最大熵模型原理 深入机器学习系列21-最大熵模型 二十一.最大熵模型原理 最大熵模型中的对数似然函数表示法解释 最大熵模型中的对数似然函数的解释   【最大熵模型的导出】判别分类模型">
<meta name="keywords" content="人工智能,机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="最大熵模型">
<meta property="og:url" content="https://alex-mcavoy.github.io/artificial-intelligence/machine-learning/ba8a14c4.html">
<meta property="og:site_name" content="Alex_McAvoy">
<meta property="og:description" content="Reference  详解最大熵模型 最大熵模型原理 深入机器学习系列21-最大熵模型 二十一.最大熵模型原理 最大熵模型中的对数似然函数表示法解释 最大熵模型中的对数似然函数的解释   【最大熵模型的导出】判别分类模型">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://alex-mcavoy.github.io/images/artificial-intelligence/machine-learning/27-1.png">
<meta property="og:image" content="https://alex-mcavoy.github.io/images/artificial-intelligence/machine-learning/27-2.png">
<meta property="og:updated_time" content="2023-03-31T12:33:44.416Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="最大熵模型">
<meta name="twitter:description" content="Reference  详解最大熵模型 最大熵模型原理 深入机器学习系列21-最大熵模型 二十一.最大熵模型原理 最大熵模型中的对数似然函数表示法解释 最大熵模型中的对数似然函数的解释   【最大熵模型的导出】判别分类模型">
<meta name="twitter:image" content="https://alex-mcavoy.github.io/images/artificial-intelligence/machine-learning/27-1.png">

<link rel="canonical" href="https://alex-mcavoy.github.io/artificial-intelligence/machine-learning/ba8a14c4.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>最大熵模型 | Alex_McAvoy</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">

  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Alex_McAvoy</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">想要成为渔夫的猎手</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


	
	
    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://alex-mcavoy.github.io/artificial-intelligence/machine-learning/ba8a14c4.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/assets/img/head.jpg">
      <meta itemprop="name" content="Alex_McAvoy">
      <meta itemprop="description" content="想要成为渔夫的猎手">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alex_McAvoy">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          最大熵模型
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-07-19 13:40:00" itemprop="dateCreated datePublished" datetime="2019-07-19T13:40:00+08:00">2019-07-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/artificial-intelligence/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/artificial-intelligence/machine-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>17 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>Reference</p>
<ul>
<li><a href="https://blog.csdn.net/ccblogger/article/details/81843304" target="_blank" rel="noopener">详解最大熵模型</a></li>
<li><a href="https://xueqiu.com/8566534281/151009557" target="_blank" rel="noopener">最大熵模型原理</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/29978153" target="_blank" rel="noopener">深入机器学习系列21-最大熵模型</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/181591842" target="_blank" rel="noopener">二十一.最大熵模型原理</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/83871690#:~:text=%E4%B9%A6%E4%B8%AD%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E8%BF%99%E9%83%A8%E5%88%86%E4%B8%AD%EF%BC%8C%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E7%9A%84%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%E8%A1%A8%E7%A4%BA%E4%B8%BA%20L_%20%7Btilde%20%7Bp%7D%7D%20%28p_%20%7Bw%7D%29%3Dlogprod_%20%7Bx%2Cy%7Dp%20%28y%7Cx%29%5E,%7Btilde%20%7Bp%7D%20%28x%2Cy%29%7D%3Dsum_%20%7Bx%2Cy%7Dtilde%20%7Bp%7D%20%28x%2Cy%29logp%20%28y%7Cx%29%20tag1" target="_blank" rel="noopener">最大熵模型中的对数似然函数表示法解释</a></li>
<li><a href="https://blog.csdn.net/wkebj/article/details/77965714" target="_blank" rel="noopener">最大熵模型中的对数似然函数的解释</a></li>
</ul>
</blockquote>
<h1 id="【最大熵模型的导出】"><a href="#【最大熵模型的导出】" class="headerlink" title="【最大熵模型的导出】"></a>【最大熵模型的导出】</h1><h2 id="判别分类模型"><a href="#判别分类模型" class="headerlink" title="判别分类模型"></a>判别分类模型</h2><p>假设分类模型为一判别模型，选用条件概率分布 $P(Y|X)$ 作为预测模型，$X\in\mathcal{X}\subseteq \mathbb{R}^n $ 为输入，$Y\in\mathcal{Y}\subseteq \mathbb{R}^n $ 为输出</p>
<p>该模型的含义为：对于给定的输入 $X$，以 $P(Y|X)$ 的概率输出 $Y$</p>
<h2 id="特征函数"><a href="#特征函数" class="headerlink" title="特征函数"></a>特征函数</h2><p><strong>特征函数（Feature Function）</strong>用于描述输入 $x$ 与输出 $y$ 之间的某一事实，其定义为：</p>
<script type="math/tex; mode=display">
f(x,y)=\left\{\begin{array}{rl}
1,& x,y\:\:满足某一事实 \\
0,& 否则
\end{array}\right.</script><p>其是一个二值函数，用于将数据集数学化</p>
<p>举例来说，对于如下数据集：</p>
<p><img width="150" src="/images/artificial-intelligence/machine-learning/27-1.png"></p>
<p>取第一列为 $x$，第二列为 $y$，可以为该数据集写出一些特征函数，例如：</p>
<script type="math/tex; mode=display">
f(x,y)=\left\{\begin{array}{rl}
1,& if:\:x=cloudy\:\:and\:\:y=outdoor \\
0,& else
\end{array}\right.</script><p>如此，可以为每个 <code>&lt;特征，标签&gt;</code> 对都做一个如上的特征函数，以实现数据集数学化</p>
<h2 id="经验分布"><a href="#经验分布" class="headerlink" title="经验分布"></a>经验分布</h2><p>对于给定训练集，可将训练数据当做由随机变量 $(X,Y)$ 产生的，那么可根据训练数据来确定联合分布 $P(X,Y)$ 的经验分布 $\tilde{P}(X,Y)$ 与边缘分布 $P(X)$ 的经验分布 $\tilde{P}(X)$</p>
<p>记 $v(X=x,Y=y)$ 为训练集中样本 $(x,y)$ 的出现频数，$v(X=x)$ 为训练集中输入 $x$ 的出现频数，$n$ 为样本容量，则对于两个经验分布，有：</p>
<script type="math/tex; mode=display">
\begin{gather*}
\tilde{P}(X=x,Y=y)=\frac{v(X=x,Y=y)}{n} \\
\tilde{P}(X=x)=\frac{v(X=x)}{n}
\end{gather*}</script><h2 id="约束条件"><a href="#约束条件" class="headerlink" title="约束条件"></a>约束条件</h2><p>对于来自参数空间 $\mathcal{X}$ 的离散随机变量 $X$，其概率分布为：</p>
<script type="math/tex; mode=display">
P(X=x_i)=p_i,\quad i=1,2,...,n</script><p>对于随机变量 $X$，其数学期望为 $E(X)=\sum\limits_{i=1}^nx_ip_i$，若 $Y=f(X)$，则关于 $X$ 的函数 $Y$ 的期望为：</p>
<script type="math/tex; mode=display">
E(Y)=\sum_{i=1}^nf(x_i)p_i</script><p>按照期望的定义，对于任意的特征函数 $f(x,y)$，用 $E_{\tilde{p}}(f)$ 表示特征函数 $f(x,y)$ 关于经验分布 $\tilde{P}(X,Y)$ 的期望，有：</p>
<script type="math/tex; mode=display">
E_{\tilde{p}}(f)=\sum_{x\in X}\sum_{y\in Y}\tilde{p}(x,y)f(x,y)</script><p>对于任意的特征函数 $f(x,y)$，用 $E_p(f)$ 表示特征函数 $f(x,y)$ 关于判别模型 $P(Y|X)$ 的期望，有：</p>
<script type="math/tex; mode=display">
E_p(f)=\sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p(y|x)f(x,y)</script><p>对于判别模型 $P(Y|X)$，希望关于经验分布 $\tilde{P}(X,Y)$ 的期望 $E_{\tilde{p}}(f)$ 应该与从训练集中得到的关于经验分布 $\tilde{P}(X)$ 的期望 $E_p(f)$ 是一致的，为此，提出特征约束：</p>
<script type="math/tex; mode=display">
E_p(f)=E_{\tilde{p}}(f)</script><p>即：</p>
<script type="math/tex; mode=display">
\sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p(y|x)f(x,y)=\sum_{x\in X}\sum_{y\in Y}\tilde{p}(x,y)f(x,y)</script><p>上式即为最大熵模型中要满足的约束条件</p>
<p>需要注意的是，由于特征函数是对数据集实现数学化的，每个 <code>&lt;特征，标签&gt;</code> 对都会做一个特征函数，因此，若有 $m$ 个特征的话，就有 $m$ 个特征函数 $f_j(x,y)$ ，相应地，有 $m$ 个约束条件</p>
<p>满足所有约束条件的模型集合为：</p>
<script type="math/tex; mode=display">
\mathcal{C}\equiv\{p\in\mathcal{P}|E_p(f_j)=E_{\tilde{p}}(f_j),j=1,2,...,m\}</script><h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h2><p>对于给定的训练集 $T$，目标是根据<a href="https://alex-mcavoy.github.io/mathematics/information-theory/81ddb601.html">最大熵原理</a>，选择一个最优的分类器</p>
<p>对于从训练集获得的特征函数和约束条件，将信息熵的概念应用到条件分布中，条件概率分布 $p(Y|X)$ 上的条件熵为：</p>
<script type="math/tex; mode=display">
H(Y|X) = -\sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p(y|x)\log p(y|x)</script><p>此时，在约束条件的模型集合 $\mathcal{C}$ 中，条件熵最大的模型，即为最大熵模型</p>
<p>要注意的是，式中的对数取自然对数</p>
<h1 id="【最大熵模型】"><a href="#【最大熵模型】" class="headerlink" title="【最大熵模型】"></a>【最大熵模型】</h1><h2 id="约束最优化问题"><a href="#约束最优化问题" class="headerlink" title="约束最优化问题"></a>约束最优化问题</h2><p>最大熵模型的学习过程，即求解最大熵模型的过程，其可以形式化为约束最优化问题</p>
<p>对于给定的训练数据集 $T=\{(\mathbf{x_1},y_1),(\mathbf{x_2},y_2),…,(\mathbf{x_n},y_n)\}$，第 $i$ 组样本中的输入 $\mathbf{x_i}$ 具有 $m$ 个特征值，即：$\mathbf{x_i} =(x_i^{(1)},x_i^{(2)},…,x_i^{(m)})\in \mathbb{R}^m$，输出 $y_i\in\mathcal{Y}= \{c_1,c_2,…,c_K\}$ 为实例的类别，特征函数 $f_j(x^{(j)},y),j=1,2,…,m$</p>
<p>最大熵模型的学习等价于下述的约束最优化问题，即求取最大的条件熵使得边际概率和为 $1$，同时特征期望相同：</p>
<script type="math/tex; mode=display">
\begin{gather*}
\max_{p\in \mathcal{C}} & H(Y|X)=-\sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p(y|x)\log p(y|x) \\
s.t. & \left\{\begin{array}{rl}
\sum\limits_{y\in Y}p(y|x) &=& 1 &\\
E_p(f_j) &=& E_{\tilde{p}}(f_j), & j=1,2,...,m
\end{array}\right.
\end{gather*}</script><p>按照最优化问题的习惯，将最大化问题改写为等价的最小化问题：</p>
<script type="math/tex; mode=display">
\begin{gather*}
\min_{p\in \mathcal{C}} & -H(Y|X)=\sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p(y|x)\log p(y|x) \\
s.t. & \left\{\begin{array}{rl}
1-\sum\limits_{y\in Y}p(y|x) &=& 0&  \\
E_{\tilde{p}}(f_j) - E_p(f_j) &=& 0, & j=1,2,...,m
\end{array}\right.
\end{gather*}</script><p>此时，求解最小化问题所得出的解，即最大熵模型学习的解</p>
<h2 id="最大熵模型-1"><a href="#最大熵模型-1" class="headerlink" title="最大熵模型"></a>最大熵模型</h2><p>对于上述的约束最优化问题，使用拉格朗日乘子法进行求解，关于拉格朗日乘子法，详见：<a href="https://alex-mcavoy.github.io/mathematics/convex-optimization/763e2d04.html">拉格朗日乘子法与对偶性</a></p>
<p>首先，引入拉格朗日乘子 $\boldsymbol{\omega} = \{\omega^{(0)},\omega^{(1)},…,\omega^{(m)}\}$，并定义拉格朗日函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
L(p,\boldsymbol{\omega}) 
\equiv& -H(Y|X) + \omega^{(0)}\bigl[1-\sum_{y\in Y} p(y|x)\bigr] + \sum_{j=1}^m \omega^{(j)}\bigl[E_{\tilde{p}}(f_j)-E_p(f_j)\bigr] \\
=& \sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p(y|x)\log p(y|x) + \omega^{(0)}\bigl[1-\sum_{y\in Y} p(y|x)\bigr] \\
&+ \sum_{j=1}^m \omega^{(j)}\Big[ \sum_{x\in X}\sum_{y\in Y} \tilde{p}(x,y)f_j(x,y) - \sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p(y|x)f_j(x,y) \Big]
\end{align*}</script><p>此时，最优化的原始问题为：</p>
<script type="math/tex; mode=display">
\min_{p\in\mathcal{C}}\:\max_{\boldsymbol{\omega}}\:L(p,\boldsymbol{\omega})</script><p>其对偶问题为：</p>
<script type="math/tex; mode=display">
\max_{\boldsymbol{\omega}}\:\min_{p\in\mathcal{C}}\:L(p,\boldsymbol{\omega})</script><p>由于拉格朗日函数 $L(p,\boldsymbol{\omega})$ 是 $p$ 的凸函数，且其满足 Slater 条件，故原始问题的解与对偶问题的解是等价的，进而可以通过求解对偶问题来求解原始问题</p>
<p>对于对偶问题内部的极小化问题 $\min\limits_{p\in\mathcal{C}}\:L(p,\boldsymbol{\omega})$，其是 $\boldsymbol{\omega}$ 的函数，将其记为 $\psi(\boldsymbol{\omega})$，即对偶函数，有：</p>
<script type="math/tex; mode=display">
\psi(\boldsymbol{\omega})= \min\limits_{p\in\mathcal{C}}\:L(p,\boldsymbol{\omega}) = L(p_{\boldsymbol\omega},\boldsymbol\omega)</script><p>将对偶函数 $\psi(\boldsymbol{\omega})$ 解记为 $p_{\boldsymbol{\omega}}$，有：</p>
<script type="math/tex; mode=display">
p_{\boldsymbol{\omega}} = \arg\min_{p\in\mathcal{C}}L(p,\boldsymbol{\omega}) = p_{\boldsymbol{\omega}}(y|x)</script><p>具体地，计算 $L(p,\boldsymbol{\omega})$ 对 $p(y|x)$ 的偏导数，即：</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{\partial L(p,\boldsymbol{\omega})}{\partial p(y|x)} 
=& 
\sum_{x\in X} \tilde{p}(x) \sum_{y\in Y} \log p(y|x) + \sum_{x\in X} \tilde{p}(x) - \sum_{y\in Y} \omega^{(0)}  \\
&- \sum_{x\in X} p(x) \sum_{j=1}^m \omega^{(j)} \sum_{y \in Y} f_j(x,y) \\
=& \sum_{x\in X,y\in Y} \tilde{p}(x)\log p(y|x) + \sum_{x\in X} \tilde{p}(x) -\sum_{y\in Y} \omega^{(0)}\\
&- \sum_{x\in X,y\in Y}\tilde{p}(x)\sum_{j=1}^m \omega^{(j)}f_j(x,y) \\
=& \sum_{x\in X,y\in Y} \tilde{p}(x)\Big[ \log p(y|x) + 1 - \omega^{(0)} - \sum_{j=1}^m \omega^{(j)}f_j(x,y) \Big]
\end{align*}</script><p>令偏导数 $\frac{\partial L(p,\boldsymbol{\omega})}{\partial p(y|x)}=0$，即：</p>
<script type="math/tex; mode=display">
\sum_{x\in X,y\in Y}\tilde{p}(x)\bigl[ \log p(y|x)+1-\omega^{(0)}-\sum_{j=1}^m \omega^{(j)}f_j(x,y) \bigr]=0</script><p>在已知 $\tilde{p}(x)&gt;0$ 的情况下，有：</p>
<script type="math/tex; mode=display">
\log p(y|x)+1-\omega^{(0)}-\sum_{j=1}^m \omega^{(j)}f_j(x,y) = 0</script><p>即：</p>
<script type="math/tex; mode=display">
\log p(y|x) = \sum_{j=1}^m \omega^{(j)}f_j(x,y)+\omega^{(0)}-1</script><p>可得：</p>
<script type="math/tex; mode=display">
p(y|x) = \exp \Big[ \sum_{j=1}^m \omega^{(j)}f_j(x,y)+\omega^{(0)}-1 \Big]</script><p>即：</p>
<script type="math/tex; mode=display">
p(y|x)=\frac{\exp \Big[ \sum_{j=1}^m \omega^{(j)}f_j(x,y) \Big]}{\exp\Big[1-\omega^{(0)}\Big]}</script><p>由于 $\sum\limits_{y\in Y} p(y|x)=1$，根据<strong>等概率原则</strong>可得：</p>
<script type="math/tex; mode=display">
\exp\Big[1-\omega^{(0)}\Big] = \sum_{y\in Y} \exp \Big[ \sum_{j=1}^m \omega^{(j)}f_j(x,y) \Big]</script><p>记 $Z_{\boldsymbol\omega}(x)=\sum\limits_{y\in Y} \exp \Big[ \sum\limits_{j=1}^m \omega^{(j)}f_j(x,y) \Big]$，称为<strong>规范化因子</strong>，其具有归一化的作用，保证了 $p_{\boldsymbol{\omega}}(y|x)$ 是概率，那么有：</p>
<script type="math/tex; mode=display">
p_{\boldsymbol{\omega}}(y|x) = \frac{1}{Z_{\boldsymbol\omega}(x)} \exp\Big[\sum_{j=1}^m\omega^{(j)}f_j(x,y)\Big]</script><p>此时所得到的对偶函数 $\psi(\boldsymbol{\omega})$ 的解 $p_{\boldsymbol{\omega}}=p_{\boldsymbol{\omega}}(y|x)$ 即为<strong>最大熵模型</strong></p>
<h1 id="【最大熵模型的学习】"><a href="#【最大熵模型的学习】" class="headerlink" title="【最大熵模型的学习】"></a>【最大熵模型的学习】</h1><h2 id="对偶函数的极大化"><a href="#对偶函数的极大化" class="headerlink" title="对偶函数的极大化"></a>对偶函数的极大化</h2><p>对于最大熵模型的约束最优化问题，内部的极小化问题的求解得到了关于 $\boldsymbol{\omega}$ 的对偶函数 $\psi(\boldsymbol{\omega})$，此时，只需对最大熵模型进行学习，即对对偶问题的外部极大化问题进行处理</p>
<p>将极大化问题的解记为 $\boldsymbol{\omega}^{*}$，有：</p>
<script type="math/tex; mode=display">
\boldsymbol{\omega}^{*}=\arg \max_{\boldsymbol{\omega}} \psi(\boldsymbol{\omega})</script><p>将 $p_{\boldsymbol{\omega}}(y|x)$ 代入到对偶函数 $\psi(\boldsymbol{\omega})$ 中，可得：</p>
<script type="math/tex; mode=display">
\begin{align*}
\psi(\boldsymbol{\omega}) 
=& \sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p(y|x)\log p(y|x) + \omega^{(0)}\bigl[1-\sum_{y\in Y} p(y|x)\bigr] \\
&+ \sum_{j=1}^m \omega^{(j)}\Big[ \sum_{x\in X}\sum_{y\in Y} \tilde{p}(x,y)f_j(x,y) - \sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p(y|x)f_j(x,y) \Big] \\
=& \sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p_{\boldsymbol{\omega}}(y|x)\log p_{\boldsymbol{\omega}}(y|x) + \omega^{(0)}\bigl[1-\sum_{y\in Y} p_{\boldsymbol{\omega}}(y|x)\bigr] \\
&+ \sum_{j=1}^m \omega^{(j)}\Big[ \sum_{x\in X}\sum_{y\in Y} \tilde{p}(x,y)f_j(x,y) - \sum_{x\in X}\tilde{p}(x)\sum_{y\in Y}p_{\boldsymbol{\omega}}(y|x)f_j(x,y) \Big] 
\end{align*}</script><p>由于 $\sum\limits_{y\in Y} p(y|x)=1$，故有：</p>
<script type="math/tex; mode=display">
\begin{align*}
\psi(\boldsymbol{\omega})
=& \sum_{x\in X,y\in Y} \tilde{p}(x) \log p_{\boldsymbol{\omega}}(y|x) + \sum_{j=1}^m \omega^{(j)} \sum_{x\in X,y\in Y} \tilde{p}(x,y)f_j(x,y) \\\
&- \sum_{j=1}^m\omega^{(j)}\sum_{x\in X,y\in Y} \tilde{p}(x)f_j(x,y)
\end{align*}</script><p>又因为：</p>
<script type="math/tex; mode=display">
\log p_{\boldsymbol{\omega}}(y|x) = \sum_{j=1}^m \omega^{(j)}f_j(x,y) - \log Z_{\boldsymbol{\omega}} (x)</script><p>则有：</p>
<script type="math/tex; mode=display">
\begin{align*}
\psi(\boldsymbol{\omega}) 
=& \sum_{x\in X,y\in Y} \tilde{p}(x) \Big[ \sum_{j=1}^m \omega^{(j)}f_j(x,y) - \log Z_{\boldsymbol{\omega}} (x) \Big] \\
&+ \sum_{j=1}^m \omega^{(j)} \sum_{x\in X,y\in Y} \tilde{p}(x,y)f_j(x,y) - \sum_{j=1}^m\omega^{(j)}\sum_{x\in X,y\in Y} \tilde{p}(x)f_j(x,y)
\end{align*}</script><p>化简得：</p>
<script type="math/tex; mode=display">
\psi(\boldsymbol{\omega}) = \sum_{x\in X,y\in Y} \tilde{p}(x,y)\sum_{j=1}^m \omega^{(j)} f_j(x,y)  - \sum_{x\in X} \tilde{p}(x) \log Z_{\boldsymbol{\omega}}(x)</script><p>故极大化问题为：</p>
<script type="math/tex; mode=display">
\boldsymbol{\omega}^{*}=\arg \max_{\boldsymbol{\omega}} \Big [\sum_{x\in X,y\in Y} \tilde{p}(x,y)\sum_{j=1}^m \omega^{(j)} f_j(x,y)  - \sum_{x\in X} \tilde{p}(x) \log Z_{\boldsymbol{\omega}}(x) \Big]</script><h2 id="最大熵模型的极大似然估计"><a href="#最大熵模型的极大似然估计" class="headerlink" title="最大熵模型的极大似然估计"></a>最大熵模型的极大似然估计</h2><p>假设样本集大小为 $n$，对于样本具体观测值 $x_1,x_2,…,x_n$，假设其取值有 $K$ 个，分别为 $v_1,v_2,…,v_K$，用 $C(X=v_i)$ 表示在观测值中样本 $v_i$ 出现的频数，那么似然函数可写为：</p>
<script type="math/tex; mode=display">
L(x_1,x_2,...,x_n;\theta) = \prod_{k=1}^K p(v_k;\theta)^{C(X=v_k)}</script><p>对上式两边同时开 $\frac{1}{n}$ 次方，可得：</p>
<script type="math/tex; mode=display">
L(x_1,x_2,...,x_n;\theta)^{\frac{1}{n}} = \prod_{k=1}^k p(v_k;\theta)^\frac{C(X=v_k)}{n}</script><p>由于经验概率 $\tilde{p}(x)=\frac{C(X=v_k)}{n}$，故有：</p>
<script type="math/tex; mode=display">
L(x_1,x_2,...,x_n;\theta)^{\frac{1}{n}} = \prod_{x\in X} p(x;\theta)^{\tilde{p}(x)}</script><p>显然，对 $L(x_1,x_2,…,x_n;\theta)^{\frac{1}{n}}$ 求极大值与对 $L(x_1,x_2,…,x_n;\theta)$ 求极大值的优化结果是相同的，那么，最终的极大似然函数可表示为：</p>
<script type="math/tex; mode=display">
L(x;\theta) = \prod_{x\in X} p(x;\theta)^{\tilde{p}(x)}</script><p>当已知训练数据的经验概率分布为 $\tilde{p}(X,Y)$ 时，有：</p>
<script type="math/tex; mode=display">
\begin{align*}
L_{\tilde{p}}
&= \log \prod_{x\in X,y\in Y} p(x,y)^{\tilde{p}(x,y)} \\
&= \sum_{x\in X,y\in Y} \tilde{p}(x,y) \log p(x,y) \\
&= \sum_{x\in X,y\in Y} \tilde{p}(x,y) \log \big[ \tilde{p}(x)p(y|x) \big] \\
&= \sum_{x\in X,y\in Y} \tilde{p}(x,y)\log p(y|x) + \sum_{x\in X,y\in Y} \tilde{p}(x,y)\log \tilde{p}(x)
\end{align*}</script><p>其中，对于第二项 $\sum\limits_{x\in X,y\in Y} \tilde{p}(x,y)\log \tilde{p}(x)$，一旦样本集确定，经验分布 $\tilde{p}(x,y)$ 与 $\tilde{p}(x)$ 可直接算出，故该项为一常数，忽略即可，故而最终的对数似然函数为：</p>
<script type="math/tex; mode=display">
L_{\tilde{p}}=\sum_{x\in X,y\in Y} \tilde{p}(x,y) \log p(y|x)</script><hr>
<p>当条件概率分布 $p(y|x)$ 为最大熵模型 $p_{\boldsymbol{\omega}}(y|x) = \frac{1}{Z_{\boldsymbol\omega}(x)} \exp\Big[\sum_{j=1}^m\omega^{(j)}f_j(x,y)\Big]$ 时，对数似然函数为：</p>
<script type="math/tex; mode=display">
\begin{align*}
L_{\tilde{p}}(p_{\boldsymbol{\omega}}) 
&= \sum_{x\in X,y\in Y} \tilde{p}(x,y)\log p_{\boldsymbol{\omega}}(y|x) \\
&= \sum_{x\in X,y\in Y} \tilde{p}(x,y)\sum_{j=1}^m \omega^{(j)} f_j(x,y)-\sum_{x\in X,y\in Y}\tilde{p}(x,y)\log Z_{\boldsymbol{\omega}} (x) \\
&= \sum_{x\in X,y\in Y} \tilde{p}(x,y)\sum_{j=1}^m \omega^{(j)} f_j(x,y)-\sum_{x\in X}\tilde{p}(x)\log Z_{\boldsymbol{\omega}} (x) \\
\end{align*}</script><p>可以发现，对数似然函数 $L_{\tilde{p}}(p_{\boldsymbol{\omega}}) $ 与对偶函数 $\psi(\boldsymbol{\omega}) $ 相等，即：</p>
<script type="math/tex; mode=display">
L_{\tilde{p}}(p_{\boldsymbol{\omega}}) = \psi(\boldsymbol{\omega})</script><p>接着，考虑对偶函数 $\psi(\boldsymbol{\omega})$，有：</p>
<script type="math/tex; mode=display">
\begin{align*}
\psi(\boldsymbol{\omega}) &= -\sum_x\tilde{p}(x)\log Z_{\boldsymbol{\omega}}(x)+\sum_{j=1}^m\omega^{(j)}E_{\tilde{p}}(f_j) \\
&= -\sum_x\tilde{p}(x)\log Z_{\boldsymbol{\omega}}(x)+\sum_{j=m}^n\omega^{(j)}\sum_{x,y}\tilde{p}(x,y)f_j(x,y)\\
&=  \sum_{x,y}\tilde{p}(x,y)\sum_{j=1}^m\omega^{(j)}f_j(x,y)-\sum_x\tilde{p}(x)\log Z_{\boldsymbol{\omega}}(x)
\end{align*}</script><p>可以发现，最大熵模型 $p_{\boldsymbol{\omega}}(y|x)$ 的对数似然函数与对偶函数 $\psi(\boldsymbol{\omega})$ 等价，即：</p>
<script type="math/tex; mode=display">
\psi(\boldsymbol{\omega})=L_{\tilde{p}}(p_{\boldsymbol{\omega}})</script><p>因此，最大熵模型学习中的对偶函数 $\psi(\boldsymbol{\omega})$ 极大化等价于最大熵模型的极大似然估计，这样对最大熵模型的学习问题就转化成了具体求解<strong>对数似然函数极大化</strong>或求解<strong>对偶函数极大化</strong>的问题，即：</p>
<script type="math/tex; mode=display">
\max_{\boldsymbol{\omega}}\sum_{x\in X,y\in Y}\tilde{p}(x,y)\sum_{j=1}^m\omega^{(j)}f_j(x,y)-\sum_{x\in X}\tilde{p}(x)\log Z_{\boldsymbol{\omega}}(x)</script><h1 id="【最大熵模型学习的最优化算法】"><a href="#【最大熵模型学习的最优化算法】" class="headerlink" title="【最大熵模型学习的最优化算法】"></a>【最大熵模型学习的最优化算法】</h1><p>由于在最大熵模型中，对偶函数的极大化等价于最大熵模型的极大似然估计，那么最大熵模型的学习问题就转换为具体求解对偶函数 $\psi(\boldsymbol{\omega})$ 极大化或对数似然函数 $L_{\tilde{p}}(p_{\boldsymbol{\omega}})$ 极大化的问题，即：</p>
<script type="math/tex; mode=display">
\boldsymbol{\omega}^{*} = \arg \max_{\boldsymbol{\omega}} \psi(\boldsymbol{\omega}) = \arg \max_{\boldsymbol{\omega}} L_{\tilde{p}}(p_{\boldsymbol{\omega}})</script><p>虽然对偶函数 $\psi(\boldsymbol{\omega})=L_{\tilde{p}}(p_{\boldsymbol{\omega}})$ 是一个光滑的凸函数，但由于其规范化因子 $Z_{\boldsymbol\omega}(x)=\sum\limits_{y\in Y} \exp \Big[ \sum\limits_{j=1}^m \omega^{(j)}f_j(x,y) \Big]$ 中存在指数函数，几乎不可能有解析解，换言之，即使有解析解，但最终仍要求数值解</p>
<p>对于上述的问题，可以使用梯度下降法、牛顿法、拟牛顿法、迭代尺度法、改进的迭代尺度法等最优化方法，其中，<strong>改进的迭代尺度法（Improved Iterative Scaling，IIS）</strong>是专门用于最大熵模型学习的最优化算法</p>
<p>关于 IIS，详见：<a href="https://alex-mcavoy.github.io/mathematics/convex-optimization/346416ed">改进的迭代尺度法</a></p>
<h1 id="【实例】"><a href="#【实例】" class="headerlink" title="【实例】"></a>【实例】</h1><p>假设随机变量 $X$ 有 5 个取值 $\{A,B,C,D,E\}$，对应取值的概率分别为 $P(A)$、$P(B)$、$P(C)$、$P(D)$、$P(E)$，存在如下约束条件：</p>
<script type="math/tex; mode=display">
\begin{gather*}
P(A)+P(B)=\frac{3}{10}\\
P(A)+P(B)+P(C)+P(D)+P(E)=1
\end{gather*}</script><p>要求学习最大熵模型</p>
<hr>
<p>为表示方便，令 $x_1$、$x_2$、$x_3$、$x_4$、$x_5$ 表示 $A$、$B$、$C$、$D$、$E$</p>
<p>于是，关于经验分布 $\tilde{P}(X)$ 的期望为：</p>
<script type="math/tex; mode=display">
E_{\tilde{p}}=p(x_1)+p(x_2)</script><p>关于模型 $P(Y|X)$ 与经验分布 $\tilde{P}(X)$ 的期望为：</p>
<script type="math/tex; mode=display">
E_p=\tilde{p}(x_1)+\tilde{p}(x_2)=\frac{3}{10}</script><p>此时，最大熵模型学习的最优化问题为：</p>
<script type="math/tex; mode=display">
\begin{gather*}
\min_{p\in\mathcal{C}} & -H(p)=\sum_{i=1}^5 p(x_i)\log p(x_i) \\
s.t. & \left\{\begin{array}{rl}
\sum\limits_{i=1}^5 p(x_i) &= \sum\limits_{i=1}^5 \tilde{p}(x_i) = 1  \\
E_{\tilde{p}}-E_p &= 0
\end{array}\right.

\end{gather*}</script><p>引入拉格朗日乘子 $\omega^{(0)}$、$\omega^{(1)}$，并定义拉格朗日函数：</p>
<script type="math/tex; mode=display">
L(p,\boldsymbol{\omega})=\sum_{i=1}^5p(x_i)\log p(x_i) + \omega^{(0)}\bigl[ 1-\sum_{i=1}^5p(x_i)\bigr] + \omega^{(1)}\bigl[ p(x_1)+p(x_2)-\frac{3}{10}\bigr]</script><p>根据拉格朗日对偶性，通过求解对偶问题来得到原始问题的解，即求解：</p>
<script type="math/tex; mode=display">
\max_{\boldsymbol{\omega}}\:\min_{p\in \mathcal{C}}\:L(p,\boldsymbol{\omega})</script><p>首先求解 $L(p,\boldsymbol{\omega})$ 关于 $p$ 的极小化问题 $\psi(\boldsymbol{\omega})$ ，求偏导数 $\frac{\partial L(p,\boldsymbol{\omega})}{\partial p(x)}$，有：</p>
<script type="math/tex; mode=display">
\begin{gather*}
\frac{\partial L(p,\boldsymbol{\omega})}{\partial p(x_1)} &=& \log p(x_1)+1 - \omega^{(0)} + \omega^{(1)} \\
\frac{\partial L(p,\boldsymbol{\omega})}{\partial p(x_2)} &=& \log p(x_2)+1 - \omega^{(0)} + \omega^{(1)} \\
\frac{\partial L(p,\boldsymbol{\omega})}{\partial p(x_3)} &=& \log p(x_3)+1 - \omega^{(0)} \\
\frac{\partial L(p,\boldsymbol{\omega})}{\partial p(x_4)} &=& \log p(x_4)+1 - \omega^{(0)} \\
\frac{\partial L(p,\boldsymbol{\omega})}{\partial p(x_5)} &=& \log p(x_5)+1 - \omega^{(0)}
\end{gather*}</script><p>令各项偏导数为 $0$，有：</p>
<script type="math/tex; mode=display">
\begin{gather*}
p(x_1)=p(x_2)=e^{\omega^{(0)}-\omega^{(1)}-1}\\
p(x_3)=p(x_4)=p(x_5)=e^{\omega^{(0)}-1}
\end{gather*}</script><p>故对偶函数为：</p>
<script type="math/tex; mode=display">
\psi(\boldsymbol{\omega})=-2e^{\omega^{(0)}-\omega^{(1)}-1}-3e^{\omega^{(0)}-1}-\frac{3}{10}\omega^{(1)}+\omega^{(0)}</script><p>接着，解 $\psi(\boldsymbol{\omega})$ 的关于 $\boldsymbol{\omega}$ 的极大化问题</p>
<p>分别求 $\psi(\boldsymbol{\omega})$ 对 $\omega^{(0)}$、$\omega^{(1)}$ 的偏导，并令其为 $0$，联立后有：</p>
<script type="math/tex; mode=display">
\begin{gather*}
e^{\omega^{(0)}-\omega^{(1)}-1}=\frac{3}{20}\\
e^{\omega^{(0)}-1}=\frac{7}{30}
\end{gather*}</script><p>故可得所要求的概率分布为：</p>
<script type="math/tex; mode=display">
\begin{gather*}
p(x_1)=p(x_2)=\frac{3}{20}\\
p(x_3)=p(x_4)=p(x_5)=\frac{7}{30}
\end{gather*}</script><h1 id="【实现】"><a href="#【实现】" class="headerlink" title="【实现】"></a>【实现】</h1><p>以 <code>sklearn</code> 中的鸢尾花数据集为例，选取其后两个特征来实现最大熵模型</p>
<p><img src="/images/artificial-intelligence/machine-learning/27-2.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,accuracy_score,classification_report,precision_score,recall_score,f1_score</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征提取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_data</span><span class="params">()</span>:</span></span><br><span class="line">    iris = load_iris()  <span class="comment"># sklearn的鸢尾花数据集</span></span><br><span class="line">    <span class="comment"># iris分为三类，前50行一类，51-100行一类，101-150行一类</span></span><br><span class="line">    X = iris.data[:, [<span class="number">2</span>, <span class="number">3</span>]] <span class="comment"># 选用后两个特征作为样本特征</span></span><br><span class="line">    y = iris.target  <span class="comment">#取species列，类别</span></span><br><span class="line">    <span class="keyword">return</span> X,y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据归一化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standard_scaler</span><span class="params">(X_train,X_test)</span>:</span></span><br><span class="line">    sc = StandardScaler() <span class="comment"># 初始化一个sc对象去对数据集作变换</span></span><br><span class="line">    scaler = sc.fit(X_train) <span class="comment"># 归一化，存有计算出的均值和方差</span></span><br><span class="line">    X_train_std = scaler.transform(X_train) <span class="comment"># 利用 scaler 进行标准化</span></span><br><span class="line">    X_test_std = scaler.transform(X_test) <span class="comment"># 利用 scaler 进行标准化</span></span><br><span class="line">    <span class="keyword">return</span> X_train_std, X_test_std</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征重建</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">features_rebuild</span><span class="params">(old_features)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    最大熵模型f(x,y)中的x是一个单独的特征，不是n维特征向量</span></span><br><span class="line"><span class="string">    为此，要对每个维度加一个区分特征</span></span><br><span class="line"><span class="string">    具体来说，将原来feature的(a0,a1,a2,...)变成(0_a0,1_a1,2_a2,...)的形式</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    new_features = []</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> old_features:</span><br><span class="line">        new_feature = []</span><br><span class="line">        <span class="keyword">for</span> i, f <span class="keyword">in</span> enumerate(feature):</span><br><span class="line">            new_feature.append(str(i) + <span class="string">'_'</span> + str(f))</span><br><span class="line">        new_features.append(new_feature)</span><br><span class="line">    <span class="keyword">return</span> new_features</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaxEnt</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_params</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        self.X_ = X</span><br><span class="line">        self.Y_ = set()</span><br><span class="line"></span><br><span class="line">        self.cal_Vxy(X, Y)</span><br><span class="line"></span><br><span class="line">        self.N = len(X)                 <span class="comment"># 数据集大小</span></span><br><span class="line">        self.n = len(self.Vxy)          <span class="comment"># 数据集中样本(x,y)的个数</span></span><br><span class="line">        self.M = <span class="number">10000.0</span>                <span class="comment"># 设置IIS算法中的学习速率M</span></span><br><span class="line"></span><br><span class="line">        self.build_dict()</span><br><span class="line">        self.cal_pxy()</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 建立id:(x,y)字典和(x,y):id字典</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.id2xy = &#123;&#125;</span><br><span class="line">        self.xy2id = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, (x, y) <span class="keyword">in</span> enumerate(self.Vxy):</span><br><span class="line">            self.id2xy[i] = (x, y)</span><br><span class="line">            self.xy2id[(x, y)] = i</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 计算样本(x,y)出现的频数v(X=x,Y=y)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_Vxy</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        self.Vxy = defaultdict(int)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">            x_, y = X[i], Y[i]</span><br><span class="line">            self.Y_.add(y)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> x_:</span><br><span class="line">                self.Vxy[(x, y)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算联合分布p(X,Y)的经验分布p~(X,Y)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_pxy</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.pxy = defaultdict(float)</span><br><span class="line">        <span class="keyword">for</span> id <span class="keyword">in</span> range(self.n):</span><br><span class="line">            (x, y) = self.id2xy[id]</span><br><span class="line">            self.pxy[id] = float(self.Vxy[(x, y)]) / float(self.N)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算规范化因子Zw(x)未加和前的单项Zw(x/yi)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_Zx</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        result = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="keyword">if</span> (x,y) <span class="keyword">in</span> self.xy2id:</span><br><span class="line">                id = self.xy2id[(x, y)]</span><br><span class="line">                result += self.w[id]</span><br><span class="line">        <span class="keyword">return</span> (math.exp(result), y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算条件概率p(Y|X)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_pyx</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        pyxs = [(self.cal_Zx(X, y)) <span class="keyword">for</span> y <span class="keyword">in</span> self.Y_]</span><br><span class="line">        Zwx = sum([prob <span class="keyword">for</span> prob, y <span class="keyword">in</span> pyxs])</span><br><span class="line">        <span class="keyword">return</span> [(prob / Zwx, y) <span class="keyword">for</span> prob, y <span class="keyword">in</span> pyxs]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算未加和的前的特征函数f(x,y)关于模型p(Y|X)与经验分布p~(X)的期望值Ep(f)的单项Ep(f_i)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_Epfi</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.Epfi = [<span class="number">0.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, X <span class="keyword">in</span> enumerate(self.X_):</span><br><span class="line">            pyxs = self.cal_pyx(X)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">                <span class="keyword">for</span> pyx, y <span class="keyword">in</span> pyxs:</span><br><span class="line">                    <span class="keyword">if</span> (x,y) <span class="keyword">in</span> self.xy2id:</span><br><span class="line">                        id = self.xy2id[(x, y)]</span><br><span class="line"></span><br><span class="line">                        self.Epfi[id] += pyx * (<span class="number">1.0</span> / self.N)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># IIS算法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, Y)</span>:</span></span><br><span class="line">        self.init_params(X, Y) <span class="comment"># 初始化参数</span></span><br><span class="line">        max_iteration = <span class="number">500</span>  <span class="comment"># 设置最大迭代次数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># step1：初始化参数值wi=0</span></span><br><span class="line">        self.w = [<span class="number">0.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n)]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># step2：对每一参数进行操作</span></span><br><span class="line">        <span class="keyword">for</span> times <span class="keyword">in</span> range(max_iteration):</span><br><span class="line">            <span class="comment"># step2.a：计算δi</span></span><br><span class="line">            detas = []</span><br><span class="line">            self.cal_Epfi()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n):</span><br><span class="line">                <span class="comment"># 指定的特征函数为指示函数，因此E~p(fi)等于p(X,y)</span></span><br><span class="line">                deta = <span class="number">1</span> / self.M * math.log(self.pxy[i] / self.Epfi[i])  </span><br><span class="line">                detas.append(deta)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># step2.b：更新wi</span></span><br><span class="line">            self.w = [self.w[i] + detas[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, testset)</span>:</span></span><br><span class="line">        results = []</span><br><span class="line">        <span class="keyword">for</span> test <span class="keyword">in</span> testset:</span><br><span class="line">            result = self.cal_pyx(test)</span><br><span class="line">            results.append(max(result, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(X_train_std, y_train)</span>:</span></span><br><span class="line">    <span class="comment"># 建立最大熵模型</span></span><br><span class="line">    model = MaxEnt()</span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    model.fit(X_train_std, y_train)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评估</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimate_model</span><span class="params">(y_pred, y_test, model)</span>:</span></span><br><span class="line">    <span class="comment"># 混淆矩阵，三分类情况下，大小为 3*3</span></span><br><span class="line">    cm2 = confusion_matrix(y_test,y_pred) </span><br><span class="line">    <span class="comment"># 准确率</span></span><br><span class="line">    acc = accuracy_score(y_test,y_pred)</span><br><span class="line">    <span class="comment"># 正确分类的样本数</span></span><br><span class="line">    acc_num = accuracy_score(y_test,y_pred,normalize=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># macro 分类报告</span></span><br><span class="line">    macro_class_report = classification_report(y_test, y_pred,target_names=[<span class="string">"类0"</span>,<span class="string">"类1"</span>,<span class="string">"类2"</span>])</span><br><span class="line">    <span class="comment"># 微精确率</span></span><br><span class="line">    micro_p = precision_score(y_test,y_pred,average=<span class="string">'micro'</span>) </span><br><span class="line">    <span class="comment"># 微召回率</span></span><br><span class="line">    micro_r = recall_score(y_test,y_pred,average=<span class="string">'micro'</span>)</span><br><span class="line">    <span class="comment"># 微F1得分</span></span><br><span class="line">    micro_f1 = f1_score(y_test,y_pred,average=<span class="string">'micro'</span>) </span><br><span class="line">    </span><br><span class="line">    indicators = &#123;<span class="string">"cm2"</span>:cm2,<span class="string">"acc"</span>:acc,<span class="string">"acc_num"</span>:acc_num,<span class="string">"macro_class_report"</span>:macro_class_report,<span class="string">"micro_p"</span>:micro_p,<span class="string">"micro_r"</span>:micro_r,<span class="string">"micro_f1"</span>:micro_f1&#125;</span><br><span class="line">    <span class="keyword">return</span> indicators</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualization</span><span class="params">(X, y, classifier, test_id=None, resolution=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">150</span>):</span><br><span class="line">        X[i][<span class="number">0</span>] = X[i][<span class="number">0</span>].replace(<span class="string">"0_"</span>,<span class="string">""</span>)</span><br><span class="line">        X[i][<span class="number">1</span>] = X[i][<span class="number">1</span>].replace(<span class="string">"1_"</span>,<span class="string">""</span>)</span><br><span class="line">    X = X.astype(float)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建 color map</span></span><br><span class="line">    markers = (<span class="string">'s'</span>, <span class="string">'x'</span>, <span class="string">'o'</span>, <span class="string">'^'</span>, <span class="string">'v'</span>)</span><br><span class="line">    colors = (<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'lightgreen'</span>, <span class="string">'gray'</span>, <span class="string">'cyan'</span>)</span><br><span class="line">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 全数据集，不同类别样本点的特征作为坐标(x,y)，用不同颜色画散点图</span></span><br><span class="line">    <span class="keyword">for</span> idx, cl <span class="keyword">in</span> enumerate(np.unique(y)):</span><br><span class="line">        plt.scatter(x=X[y == cl, <span class="number">0</span>], y=X[y == cl, <span class="number">1</span>], alpha=<span class="number">0.8</span>, c=cmap(idx), marker=markers[idx], label=cl) </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 高亮测试集</span></span><br><span class="line">    <span class="keyword">if</span> test_id:</span><br><span class="line">        X_test, y_test = X[test_id, :], y[test_id]</span><br><span class="line">        <span class="comment"># c设置颜色，测试集不同类别的实例点画图不区别颜色</span></span><br><span class="line">        plt.scatter(x=X_test[:, <span class="number">0</span>], y=X_test[:, <span class="number">1</span>], alpha=<span class="number">1.0</span>, c=<span class="string">'gray'</span>, marker=<span class="string">'^'</span>, linewidths=<span class="number">1</span>, s=<span class="number">55</span>, label=<span class="string">'test set'</span>)</span><br><span class="line">        </span><br><span class="line">    plt.xlabel(<span class="string">'petal length [standardized]'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'petal width [standardized]'</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 特征提取</span></span><br><span class="line">    X, y = deal_data()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 简单交叉验证</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据标准化</span></span><br><span class="line">    X_train_std, X_test_std = standard_scaler(X_train, X_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征重建</span></span><br><span class="line">    X_train_std = features_rebuild(X_train_std)</span><br><span class="line">    X_test_std = features_rebuild(X_test_std)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    model = train_model(X_train_std, y_train)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测结果</span></span><br><span class="line">    y_pred = model.predict(X_test_std)</span><br><span class="line">    print(<span class="string">"y test:"</span>, y_test) <span class="comment"># 测试集y值</span></span><br><span class="line">    print(<span class="string">"y pred:"</span>, y_pred) <span class="comment"># 预测y值</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型评估</span></span><br><span class="line">    indicators = estimate_model(y_pred, y_test, model)</span><br><span class="line">    cm2 = indicators[<span class="string">"cm2"</span>] </span><br><span class="line">    print(<span class="string">"混淆矩阵：\n"</span>, cm2) </span><br><span class="line">    acc =  indicators[<span class="string">"acc"</span>]</span><br><span class="line">    print(<span class="string">"准确率："</span>, acc)</span><br><span class="line">    acc_num =  indicators[<span class="string">"acc_num"</span>]</span><br><span class="line">    print(<span class="string">"正确分类的样本数："</span>, acc_num)</span><br><span class="line">    macro_class_report = indicators[<span class="string">"macro_class_report"</span>]</span><br><span class="line">    print(<span class="string">"macro 分类报告：\n"</span>, macro_class_report)</span><br><span class="line">    micro_p = indicators[<span class="string">"micro_p"</span>]</span><br><span class="line">    print(<span class="string">"微精确率："</span>, micro_p)</span><br><span class="line">    micro_r = indicators[<span class="string">"micro_r"</span>]</span><br><span class="line">    print(<span class="string">"微召回率："</span>, micro_r)</span><br><span class="line">    micro_f1 = indicators[<span class="string">"micro_f1"</span>]</span><br><span class="line">    print(<span class="string">"微F1得分："</span>, micro_f1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 可视化</span></span><br><span class="line">    X_combined_std = np.vstack((X_train_std, X_test_std))    </span><br><span class="line">    y_combined = np.hstack((y_train, y_test))</span><br><span class="line">    <span class="comment"># classifier为分类器，test_id为测试集序号</span></span><br><span class="line">    visualization(X_combined_std, y_combined, classifier=model, test_id=range(<span class="number">105</span>, <span class="number">150</span>))</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        <div class="reward-container">
  <div>感谢您对我的支持，让我继续努力分享有用的技术与知识点！</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/assets/img/wechatpay.jpg" alt="Alex_McAvoy 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/assets/img/alipay.jpg" alt="Alex_McAvoy 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Alex_McAvoy
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://alex-mcavoy.github.io/artificial-intelligence/machine-learning/ba8a14c4.html" title="最大熵模型">https://alex-mcavoy.github.io/artificial-intelligence/machine-learning/ba8a14c4.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/artificial-intelligence/" rel="tag"># 人工智能</a>
              <a href="/tags/machine-learning/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/mathematics/information-theory/81ddb601.html" rel="prev" title="最大熵原理">
      <i class="fa fa-chevron-left"></i> 最大熵原理
    </a></div>
      <div class="post-nav-item">
    <a href="/mathematics/convex-optimization/346416ed.html" rel="next" title="改进的迭代尺度法(IIS)">
      改进的迭代尺度法(IIS) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#【最大熵模型的导出】"><span class="nav-number">1.</span> <span class="nav-text">【最大熵模型的导出】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#判别分类模型"><span class="nav-number">1.1.</span> <span class="nav-text">判别分类模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征函数"><span class="nav-number">1.2.</span> <span class="nav-text">特征函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#经验分布"><span class="nav-number">1.3.</span> <span class="nav-text">经验分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#约束条件"><span class="nav-number">1.4.</span> <span class="nav-text">约束条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最大熵模型"><span class="nav-number">1.5.</span> <span class="nav-text">最大熵模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【最大熵模型】"><span class="nav-number">2.</span> <span class="nav-text">【最大熵模型】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#约束最优化问题"><span class="nav-number">2.1.</span> <span class="nav-text">约束最优化问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最大熵模型-1"><span class="nav-number">2.2.</span> <span class="nav-text">最大熵模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【最大熵模型的学习】"><span class="nav-number">3.</span> <span class="nav-text">【最大熵模型的学习】</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#对偶函数的极大化"><span class="nav-number">3.1.</span> <span class="nav-text">对偶函数的极大化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最大熵模型的极大似然估计"><span class="nav-number">3.2.</span> <span class="nav-text">最大熵模型的极大似然估计</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【最大熵模型学习的最优化算法】"><span class="nav-number">4.</span> <span class="nav-text">【最大熵模型学习的最优化算法】</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【实例】"><span class="nav-number">5.</span> <span class="nav-text">【实例】</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#【实现】"><span class="nav-number">6.</span> <span class="nav-text">【实现】</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">  
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Alex_McAvoy"
      src="/assets/img/head.jpg">
  <p class="site-author-name" itemprop="name">Alex_McAvoy</p>
  <div class="site-description" itemprop="description">想要成为渔夫的猎手</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">640</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">71</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Alex-McAvoy" title="GitHub → https://github.com/Alex-McAvoy" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/u011815404" title="CSDN → https://blog.csdn.net/u011815404" rel="noopener" target="_blank"><i class="fas fa-copyright fa-fw"></i>CSDN</a>
      </span>
  </div>

<!-- 访客地图 -->
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=50n58yo58ow&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;s=140" async="async"></script>



  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/sdz20172133" title="https://blog.csdn.net/sdz20172133" rel="noopener" target="_blank">神仙队友</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://pxlsdz.gitee.io/" title="https://pxlsdz.gitee.io/" rel="noopener" target="_blank">神仙队友blog2</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/sdau_fangshifeng" title="https://blog.csdn.net/sdau_fangshifeng" rel="noopener" target="_blank">酷头</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://bycore.net" title="https://bycore.net" rel="noopener" target="_blank">凉了的某饼同学</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://kris-cn.github.io/" title="http://kris-cn.github.io/" rel="noopener" target="_blank">翟孙</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://deng.fun/" title="http://deng.fun/" rel="noopener" target="_blank">芙蓉姐姐</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/wentong_Xu" title="https://blog.csdn.net/wentong_Xu" rel="noopener" target="_blank">小黑贱胖子</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.yuheng.tech/" title="https://www.yuheng.tech/" rel="noopener" target="_blank">咸于</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/lanshan1111" title="https://blog.csdn.net/lanshan1111" rel="noopener" target="_blank">肥硕硕</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/sdauguanweihong" title="https://blog.csdn.net/sdauguanweihong" rel="noopener" target="_blank">管少</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/qq_41661919" title="https://blog.csdn.net/qq_41661919" rel="noopener" target="_blank">俊杰</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/qq_40859951" title="https://blog.csdn.net/qq_40859951" rel="noopener" target="_blank">峰哥</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/LSD20164388" title="https://blog.csdn.net/LSD20164388" rel="noopener" target="_blank">狗冬</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/qq_37868325" title="https://blog.csdn.net/qq_37868325" rel="noopener" target="_blank">妍大佬</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://sanshuiii.github.io/" title="https://sanshuiii.github.io/" rel="noopener" target="_blank">sanshuiii</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://zjpzhao.github.io/" title="https://zjpzhao.github.io/" rel="noopener" target="_blank">brain</a>
        </li>
    </ul>
  </div>

		
      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>
  


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alex_McAvoy</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

</br>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>



  




  <script src="/js/local-search.js"></script>












  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '48fd73a538d3fa927f65',
      clientSecret: '56618aefeee90bac3c20de8d51bb0b985d67252d',
      repo        : 'Gitalk-Comment',
      owner       : 'Alex-McAvoy',
      admin       : ['Alex-McAvoy'],
      id          : 'f2b944d809c48d56f9f3fdb62b86850a',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

  
  <!-- 鼠标单击粒子特效 -->
  <script type="text/javascript" src="/js/cursor-effects.js"></script>
  <!-- 多级目录 -->
  <script type="text/javascript" src="/js/category.js"></script>
  
</body>
</html>